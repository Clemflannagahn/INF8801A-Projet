{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Afin d'améliorer les résultats, transformation de la partie III.A (le passage de photo à sketch) en l'application d'un réseau de neurones (Pix2Pix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/jclohjc/portrait-to-sketch-pix2pix-conditional-gan/notebook#Model-Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from natsort import natsorted, os_sorted, ns\n",
    "import time\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification du GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is NOT available\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available\")\n",
    "    # Set TensorFlow to use GPU\n",
    "    tf.config.set_visible_devices(tf.config.list_physical_devices('GPU'), 'GPU')\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "    print(len(logical_devices), \"GPU(s) are available.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available\")\n",
    "    # If GPU is not available, use CPU\n",
    "    tf.config.set_visible_devices(tf.config.list_physical_devices('CPU'), 'CPU')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "portrait_path = r\"./data/photos/\"\n",
    "sketches_path = r\"./data/sketches/\"\n",
    "\n",
    "portrait_imgname = os.listdir(portrait_path)\n",
    "sketches_imgname = os.listdir(sketches_path)\n",
    "\n",
    "# Appariement des images pour l'entrainement\n",
    "portrait_imgname = natsorted(portrait_imgname, alg=ns.IGNORECASE)\n",
    "sketches_imgname = natsorted(sketches_imgname, alg=ns.IGNORECASE)\n",
    "\n",
    "portrait_imgs = [portrait_path+\"/\"+imgname for imgname in portrait_imgname]\n",
    "sketches_imgs = [sketches_path+\"/\"+imgname for imgname in sketches_imgname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_path):\n",
    "    \"\"\" Lecture à l'aide de tensorflow des images\"\"\"\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(image,channels=IN_CHANNEL)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image\n",
    "\n",
    "def resize(image, height, width):\n",
    "  \"\"\" Pour redimensionner les images à la taille souhaitée (ici 256x256)\"\"\"\n",
    "  image = tf.image.resize(image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  return image\n",
    "\n",
    "def normalize(image):\n",
    "  image = (image/ 127.5) - 1\n",
    "  return image\n",
    "\n",
    "def flip(image):\n",
    "    \"\"\" Data Augmentation par rotation des images\"\"\"\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "adjustment_seed = 1\n",
    "random.seed(adjustment_seed)\n",
    "def random_adjustment(image1,image2):\n",
    "    \"\"\" Data Augmentation par ajustement de la luminosité et du contraste des images\n",
    "        Nécessité de le faire directement pour les deux images pour modifier de la même manière le sketch et la photo correspondante\n",
    "    \"\"\"\n",
    "    rand_bright = random.uniform(1,5)\n",
    "    rand_contrast = random.uniform(1,2)\n",
    "    \n",
    "    image1 = tf.image.adjust_brightness(image1,delta=rand_bright)\n",
    "    image2 = tf.image.adjust_brightness(image2,delta=rand_bright)\n",
    "    \n",
    "    image1 = tf.image.adjust_contrast(image1,contrast_factor=rand_contrast)\n",
    "    image2 = tf.image.adjust_contrast(image2,contrast_factor=rand_contrast)\n",
    "    \n",
    "    image1 = tf.cast(image1, tf.float32)\n",
    "    image2 = tf.cast(image2, tf.float32)\n",
    "    \n",
    "    return image1, image2\n",
    "\n",
    "IMG_SIZE = 256\n",
    "#even though the generated image is greyscale (1channel), it is converted into RGB (3channels)\n",
    "IN_CHANNEL = 3\n",
    "OUT_CHANNEL = IN_CHANNEL #the in channel and out channel has to be the same for tf batch\n",
    "TRAIN_BATCH = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séparation en train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images in trainset: 534\n",
      "trainset len: 134\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(portrait_imgs)\n",
    "TRAIN_SIZE = int(BUFFER_SIZE*0.95)\n",
    "TEST_SIZE =  int(BUFFER_SIZE - TRAIN_SIZE)\n",
    "\n",
    "\n",
    "train_imgs_idx = []\n",
    "test_imgs_idx = []\n",
    "\n",
    "train_imgs = []\n",
    "test_imgs = []\n",
    "\n",
    "for i in range(BUFFER_SIZE):\n",
    "        portrait_img = load(portrait_imgs[i])\n",
    "        portrait_img = resize(portrait_img,IMG_SIZE,IMG_SIZE)\n",
    "        \n",
    "        sketch_img = load(sketches_imgs[i])\n",
    "        sketch_img = resize(sketch_img,IMG_SIZE,IMG_SIZE)\n",
    "        \n",
    "        # Data augmentation\n",
    "        portrait_img2 = flip(portrait_img)\n",
    "        portrait_img2 = normalize(portrait_img2)\n",
    "        \n",
    "        sketch_img2 = flip(sketch_img)\n",
    "        sketch_img2 = normalize(sketch_img2)\n",
    "        \n",
    "        portrait_img3,sketch_img3 = random_adjustment(portrait_img,sketch_img)\n",
    "        portrait_img3 = normalize(portrait_img3)\n",
    "        sketch_img3 = normalize(sketch_img3)\n",
    "        \n",
    "        portrait_img = normalize(portrait_img)\n",
    "        sketch_img = normalize(sketch_img)\n",
    "    \n",
    "        if (i<TRAIN_SIZE):\n",
    "            train_imgs.append(tf.convert_to_tensor([portrait_img,sketch_img]))\n",
    "            train_imgs.append(tf.convert_to_tensor([portrait_img2,sketch_img2]))\n",
    "            train_imgs.append(tf.convert_to_tensor([portrait_img3,sketch_img3]))\n",
    "        else:\n",
    "            test_imgs.append(tf.convert_to_tensor([portrait_img,sketch_img]))\n",
    "            test_imgs.append(tf.convert_to_tensor([portrait_img2,sketch_img2]))\n",
    "            test_imgs.append(tf.convert_to_tensor([portrait_img3,sketch_img3]))\n",
    "\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(train_imgs))\n",
    "print(\"images in trainset:\",len(train_set))\n",
    "train_set = train_set.shuffle(len(train_set))\n",
    "    \n",
    "train_set = train_set.batch(TRAIN_BATCH)\n",
    "print(\"trainset len:\",len(train_set))\n",
    "\n",
    "# test_set = tf.data.Dataset.from_tensor_slices(test_imgs_idx)\n",
    "test_set = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(test_imgs))\n",
    "# test_set = test_set.map(dataset_process,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_set = test_set.shuffle(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du Generator du GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(feature, kernel, apply_batchnorm=False):\n",
    "    \"\"\" the output image size will always be input image size/strides as padding=\"same\" \"\"\"\n",
    "    result = tf.keras.Sequential()\n",
    "    \n",
    "    if apply_batchnorm:\n",
    "        result.add(\n",
    "            tf.keras.layers.Conv2D(feature, kernel, strides=2, padding='same', use_bias=False))\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    else:\n",
    "        result.add(\n",
    "            tf.keras.layers.Conv2D(feature, kernel, strides=2, padding='same'))\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "#the output image size will always be input image size/strides as padding=\"same\"\n",
    "def upsample(feature, kernel, apply_batchnorm=False, apply_dropout=False, dropout=0):\n",
    "    result = tf.keras.Sequential()\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(\n",
    "            tf.keras.layers.Conv2DTranspose(feature, kernel, strides=2, padding='same', use_bias=False))\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    else:\n",
    "        result.add(\n",
    "            tf.keras.layers.Conv2DTranspose(feature, kernel, strides=2, padding='same'))\n",
    "        \n",
    "    if apply_dropout:\n",
    "          result.add(tf.keras.layers.Dropout(dropout))\n",
    "            \n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "input_shape = (1, IMG_SIZE, IMG_SIZE, IN_CHANNEL) #1 represents batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[IMG_SIZE, IMG_SIZE, IN_CHANNEL])\n",
    "    \n",
    "    #the comment below is considering IMG_SIZE=256\n",
    "    down_stack = [\n",
    "        downsample(64, 4),  # (batch_size, 128, 128, 64)\n",
    "        downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "        downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "        downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "    ]\n",
    "    #the up_stack does not include the last layer of forming the desired image\n",
    "    up_stack = [\n",
    "        #upsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "        upsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "        upsample(64, 4),  # (batch_size, 128, 128, 64)\n",
    "    ]\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUT_CHANNEL, 4,\n",
    "                                           strides=2,\n",
    "                                           padding='same',\n",
    "                                           activation='tanh')  # (batch_size, 256, 256, channel)\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    #downsampling\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    #upsampling (with skip connection)\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "        \n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_35\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_35\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_21       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_22       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │ sequential_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_23       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ sequential_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_24       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │ sequential_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_25       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,408</span> │ sequential_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │ sequential_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_26       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,704</span> │ concatenate_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_10      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │ sequential_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_27       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,208</span> │ concatenate_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_11      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ sequential_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_15 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,147</span> │ concatenate_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_21       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m3,136\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_22       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m131,200\u001b[0m │ sequential_21[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_23       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │    \u001b[38;5;34m524,544\u001b[0m │ sequential_22[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_24       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │  \u001b[38;5;34m2,097,664\u001b[0m │ sequential_23[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_25       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │  \u001b[38;5;34m2,097,408\u001b[0m │ sequential_24[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ sequential_25[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m512\u001b[0m)              │            │ sequential_23[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_26       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │  \u001b[38;5;34m1,048,704\u001b[0m │ concatenate_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_10      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ sequential_26[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m256\u001b[0m)              │            │ sequential_22[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_27       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │    \u001b[38;5;34m262,208\u001b[0m │ concatenate_10[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_11      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ sequential_27[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m128\u001b[0m)              │            │ sequential_21[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_transpose_15 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │      \u001b[38;5;34m6,147\u001b[0m │ concatenate_11[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,171,011</span> (23.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,171,011\u001b[0m (23.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,171,011</span> (23.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,171,011\u001b[0m (23.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64,to_file='generator.png')\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  # Mean absolute error\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    inp = tf.keras.layers.Input(shape=[IMG_SIZE, IMG_SIZE, IN_CHANNEL], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[IMG_SIZE, IMG_SIZE, OUT_CHANNEL], name='target_image')\n",
    "    \n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, total channels)\n",
    "    \n",
    "    #get the features from the image\n",
    "    x = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "    x = downsample(128, 4)(x)  # (batch_size, 64, 64, 128)\n",
    "    x = downsample(256, 4)(x)  # (batch_size, 32, 32, 256)\n",
    "    \n",
    "    #make the output image NxN with 1 channel\n",
    "    x = tf.keras.layers.Conv2D(1, 1, strides=1, padding='valid')(x)  # (batch_size, size, size, 1)\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=x)\n",
    "\n",
    "input_shape = (1, 32, 32, 256) #1 represents batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64,to_file='discriminator.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam()#(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam()#(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_images,target_images,epoch):    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        ##TRAIN THE GENERATOR\n",
    "        gen_output = generator(input_images, training=True) #generates image\n",
    "        disc_gen_output = discriminator([input_images,gen_output], training=True) #get the disriminator output of generated image\n",
    "        gen_total_loss, _, _ = generator_loss(disc_gen_output, gen_output, target_images) #get the loss function\n",
    "        #apply gradient\n",
    "        generator_gradients = gen_tape.gradient(gen_total_loss,generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients,generator.trainable_variables))\n",
    "\n",
    "        ##TRAIN THE DISCRIMINATOR\n",
    "        disc_real_output = discriminator([input_images,target_images], training=True) #get the discriminator output of real image\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_gen_output) #get the loss function\n",
    "        #apply gradient\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n",
    "    return gen_total_loss,disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset,epochs=56):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        #train the model\n",
    "        for batch in train_dataset:\n",
    "            #convert the input images\n",
    "            input_images = []\n",
    "            target_images = []\n",
    "            #print(batch.shape)\n",
    "\n",
    "            for image_pair in batch:\n",
    "                input_image = image_pair[0]\n",
    "                target_image = image_pair[1]\n",
    "                #form the input images\n",
    "                input_images.append(input_image)\n",
    "                \n",
    "                #for the target images\n",
    "                target_images.append(target_image)\n",
    "\n",
    "            input_images = tf.convert_to_tensor(input_images)\n",
    "            target_images = tf.convert_to_tensor(target_images)\n",
    "\n",
    "            gen_loss, disc_loss = train_step(input_images,target_images,epoch)\n",
    "        \n",
    "        if (epoch%5==0):\n",
    "            print(\"epoch {} gen loss:{}, disc loss:{}\".format(epoch,gen_loss, disc_loss))\n",
    "            print(\"time taken for 5 epoch:{}s\".format(time.time()-start))\n",
    "            start = time.time()\n",
    "\n",
    "train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(image):\n",
    "  image = (image+1)\n",
    "  image = image*127.5\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_set))\n",
    "temp_img_num = int(len(test_set)/2)\n",
    "temp_img_num = 5\n",
    "\n",
    "test_set1 = test_set.take(temp_img_num)\n",
    "test_set2 = test_set.take(temp_img_num)\n",
    "\n",
    "for step, (input_image, target_image) in test_set1.enumerate():\n",
    "    ax = plt.subplot(2,temp_img_num,int(step)+1)\n",
    "    plt.imshow(input_image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    gen_output = generator(input_image[tf.newaxis, ...], training=False)\n",
    "    ax = plt.subplot(2,temp_img_num,temp_img_num+int(step)+1)\n",
    "    \n",
    "    gen_output = gen_output[0,...]\n",
    "    denorm_output = denormalize(gen_output)\n",
    "    denorm_output = tf.cast(denorm_output,tf.int32)\n",
    "    denorm_output = tf.clip_by_value(denorm_output,0,255)\n",
    "\n",
    "    plt.imshow(denorm_output)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (input_image, target_image) in test_set2.enumerate():\n",
    "    ax = plt.subplot(2,temp_img_num,int(step)+1)\n",
    "    plt.imshow(input_image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    gen_output = generator(input_image[tf.newaxis, ...], training=False)\n",
    "    ax = plt.subplot(2,temp_img_num,temp_img_num+int(step)+1)\n",
    "    \n",
    "    gen_output = gen_output[0,...]\n",
    "    denorm_output = denormalize(gen_output)\n",
    "    denorm_output = tf.cast(denorm_output,tf.int32)\n",
    "    denorm_output = tf.clip_by_value(denorm_output,0,255)\n",
    "\n",
    "    plt.imshow(denorm_output)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"PortraitToSketch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF8111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
